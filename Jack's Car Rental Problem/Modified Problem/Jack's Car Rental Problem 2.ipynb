{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Jack's Car Rental Problem (2nd Version) </strong>\n",
    "This notebook provides a solution for the modified version of \"Jack's Car Rental Problem\" presented as \"Exercise 4.7\" in the \"Reinforcement Learning: An Introduction, Second Edition\" book by Sutton and Barto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <strong> Original Problem Description </strong>\n",
    "Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited 10 dollars by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of 2 dollars per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables. Suppose λ (parameter for poisson process) is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be γ = 0.9 and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight.\n",
    "\n",
    "##### <strong> Modification </strong>\n",
    "One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Location:\n",
    "    def __init__(self, capacity, lambda_request, lambda_return, epsilon = 0.005):\n",
    "        self.capacity = capacity\n",
    "        self.lambda_request = lambda_request\n",
    "        self.lambda_return = lambda_return\n",
    "        self.epsilon = epsilon\n",
    "        self.request_seq = self.__request_count()\n",
    "        self.return_seq = self.__return_count()\n",
    "\n",
    "    def __poisson_pmf(x, lam):\n",
    "        return lam**x * math.exp(-lam) / math.factorial(x)\n",
    "\n",
    "    def __request_count(self):\n",
    "        count = 0\n",
    "        prob = Location.__poisson_pmf(count, self.lambda_request)\n",
    "        res = []\n",
    "        prob_sum = 0.0\n",
    "\n",
    "        while prob < self.epsilon:\n",
    "            count += 1\n",
    "            prob = Location.__poisson_pmf(count, self.lambda_request)\n",
    "\n",
    "\n",
    "        while prob >= self.epsilon:\n",
    "            prob_sum += prob\n",
    "            res.append((count, prob))\n",
    "            count += 1\n",
    "            prob = Location.__poisson_pmf(count, self.lambda_request)\n",
    "\n",
    "        res = [(count, prob / prob_sum) for count, prob in res]\n",
    "\n",
    "        return res\n",
    "\n",
    "    def __return_count(self):\n",
    "        count = 0\n",
    "        prob = Location.__poisson_pmf(count, self.lambda_return)\n",
    "        res = []\n",
    "        prob_sum = 0.0\n",
    "\n",
    "        while prob < self.epsilon:\n",
    "            count += 1\n",
    "            prob = Location.__poisson_pmf(count, self.lambda_return)\n",
    "    \n",
    "        while prob >= self.epsilon:\n",
    "            prob_sum += prob\n",
    "            res.append((count, prob))\n",
    "            count += 1\n",
    "            prob = Location.__poisson_pmf(count, self.lambda_return)\n",
    "\n",
    "        res = [(count, prob / prob_sum) for count, prob in res]\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def request_count(self):\n",
    "        return self.request_seq\n",
    "    \n",
    "    def return_count(self):\n",
    "        return self.return_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.actions = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "        self.states = []\n",
    "        self.location1 = Location(20, 3, 3)\n",
    "        self.location2 = Location(20, 4, 2)\n",
    "        self.rental_price = 10.0\n",
    "        self.move_cost = -2.0\n",
    "\n",
    "        for i in range(self.location1.capacity + 1):\n",
    "            for j in range(self.location2.capacity + 1):\n",
    "                self.states.append((i, j))\n",
    "    \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def __get_reward(self, state, action, loc1_req_count, loc2_req_count, loc1_ret_count, loc2_ret_count):\n",
    "        reward = 0\n",
    "\n",
    "        reward += self.move_cost * ((action - 1) if action > 0 else abs(action))\n",
    "\n",
    "        reward += (-4) if (state[0] - action > 10) else 0\n",
    "        reward += (-4) if (state[1] + action > 10) else 0\n",
    "\n",
    "        reward += self.rental_price * min(min(state[0] - action, 20), loc1_req_count)\n",
    "        reward += self.rental_price * min(min(state[1] + action, 20), loc2_req_count)\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def get_nextState_reward_prob(self, state, action):\n",
    "\n",
    "        for loc1_req_count, loc1_req_prob in self.location1.request_count():\n",
    "            for loc2_req_count, loc2_req_prob in self.location2.request_count():\n",
    "                for loc1_ret_count, loc1_ret_prob in self.location1.return_count():\n",
    "                    for loc2_ret_count, loc2_ret_prob in self.location2.return_count():\n",
    "                        i = min(max(min(state[0] - action, self.location1.capacity) - loc1_req_count, 0) + loc1_ret_count, self.location1.capacity)\n",
    "                        j = min(max(min(state[1] + action, self.location2.capacity) - loc2_req_count, 0) + loc2_ret_count, self.location2.capacity)\n",
    "\n",
    "                        reward = self.__get_reward(state, action, loc1_req_count, loc2_req_count, loc1_ret_count, loc2_ret_count)\n",
    "                        prob = loc1_req_prob * loc2_req_prob * loc1_ret_prob * loc2_ret_prob\n",
    "                        yield (int(i), int(j)), reward, prob\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid(state, action):\n",
    "        return ((state[0] - action - 1) >= 0) and ((state[1] + action + 1) >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, discount = 0.9):\n",
    "        self.env = env\n",
    "        self.V = None\n",
    "        self.policy = None\n",
    "        self.discount = discount\n",
    "        self.counter = 1\n",
    "\n",
    "    def policy_iteration(self, theta = 0.01):\n",
    "        self.V = np.zeros((self.env.location1.capacity + 1, self.env.location2.capacity + 1))\n",
    "        self.policy = np.zeros((self.env.location1.capacity + 1, self.env.location2.capacity + 1))\n",
    "        self.counter = 1\n",
    "\n",
    "        while True:\n",
    "            prev_V = np.copy(self.V)\n",
    "\n",
    "            self.policy_evaluation(theta)\n",
    "            \n",
    "            stable_policy = self.policy_improvement()\n",
    "\n",
    "            if np.greater(self.V, prev_V).all():\n",
    "                print(\"State Value Improved!\")\n",
    "            else:\n",
    "                print(\"State Value did not Improve!\")\n",
    "\n",
    "            self.save_state_value_function(\".\\\\outputs\\\\state_value_functions\\\\V\" + str(self.counter) + \".png\")\n",
    "            self.save_policy(\".\\\\outputs\\\\policies\\\\p\" + str(self.counter) + \".png\")\n",
    "            self.counter += 1 \n",
    "            \n",
    "            if stable_policy:\n",
    "                print(\"-------- Policy Iteration Done! --------\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"-------- Policy Iteration \" + str(self.counter) + \" --------\")\n",
    "            \n",
    "    def policy_evaluation(self, theta):\n",
    "\n",
    "        while True:\n",
    "            delta = 0\n",
    "\n",
    "            for s in self.env.get_states():\n",
    "                v = self.V[s[0], s[1]]\n",
    "                tmp = 0\n",
    "                for next_s, reward, prob in self.env.get_nextState_reward_prob(s, self.policy[s[0], s[1]]):\n",
    "                    tmp += prob * (reward + self.discount * self.V[next_s[0], next_s[1]])\n",
    "                self.V[s[0], s[1]] = tmp\n",
    "                delta = max(delta, abs(v - self.V[s[0], s[1]]))\n",
    "            \n",
    "            print(\"---- Delta: \", delta)\n",
    "            if delta < theta:\n",
    "                print(\"Policy Evaluation Done!\")\n",
    "                break\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        for s in self.env.get_states():\n",
    "            old_action = self.policy[s[0], s[1]]\n",
    "\n",
    "            best_action = old_action\n",
    "            best_action_value = -1e5\n",
    "\n",
    "            for action in self.env.get_actions():\n",
    "                \n",
    "                if self.env.is_valid(s, action):\n",
    "                    action_value = 0\n",
    "                    for next_s, reward, prob in self.env.get_nextState_reward_prob(s, action):\n",
    "                        action_value += prob * (reward + self.discount * self.V[next_s[0], next_s[1]])\n",
    "                \n",
    "                    if action_value > best_action_value:\n",
    "                        best_action = action\n",
    "                        best_action_value = action_value\n",
    "            \n",
    "            self.policy[s[0], s[1]] = best_action\n",
    "            \n",
    "            if old_action != self.policy[s[0], s[1]]:\n",
    "                policy_stable = False\n",
    "\n",
    "        return policy_stable\n",
    "    \n",
    "    def save_state_value_function(self, filename):\n",
    "        plt.imshow(self.V)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(filename)\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    def save_policy(self, filename):\n",
    "        plt.imshow(self.policy)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(filename)\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Delta:  171.86254673870607\n",
      "---- Delta:  120.31289055878005\n",
      "---- Delta:  82.38775781013965\n",
      "---- Delta:  65.04429434827725\n",
      "---- Delta:  51.91012301121762\n",
      "---- Delta:  40.44585426529238\n",
      "---- Delta:  31.643466942938062\n",
      "---- Delta:  25.14344499939739\n",
      "---- Delta:  21.02597262278755\n",
      "---- Delta:  17.621385114134284\n",
      "---- Delta:  14.738416495098647\n",
      "---- Delta:  12.304194252125228\n",
      "---- Delta:  10.25381107740418\n",
      "---- Delta:  8.530792820113675\n",
      "---- Delta:  7.0862713884503705\n",
      "---- Delta:  5.878005901564052\n",
      "---- Delta:  4.869541656074318\n",
      "---- Delta:  4.029513112318284\n",
      "---- Delta:  3.3310435865995487\n",
      "---- Delta:  2.751206679211748\n",
      "---- Delta:  2.270533318796481\n",
      "---- Delta:  1.8725597711764408\n",
      "---- Delta:  1.5434165597798142\n",
      "---- Delta:  1.2714589805644891\n",
      "---- Delta:  1.046939042558563\n",
      "---- Delta:  0.8617174594636481\n",
      "---- Delta:  0.7090132915479899\n",
      "---- Delta:  0.5831881491523632\n",
      "---- Delta:  0.47956151352468623\n",
      "Policy Evaluation Done!\n",
      "State Value Improved!\n",
      "-------- Policy Iteration 2 --------\n",
      "---- Delta:  90.76020684187415\n",
      "---- Delta:  8.988839886261076\n",
      "---- Delta:  4.452409244703858\n",
      "---- Delta:  3.5166443080797762\n",
      "---- Delta:  2.94794167976886\n",
      "---- Delta:  2.4425077334105936\n",
      "---- Delta:  2.008184723852537\n",
      "---- Delta:  1.6471451819393224\n",
      "---- Delta:  1.3496133351917479\n",
      "---- Delta:  1.1052216754086999\n",
      "---- Delta:  0.90480938575962\n",
      "---- Delta:  0.7406095417380243\n",
      "---- Delta:  0.606146268476266\n",
      "---- Delta:  0.49606576019624526\n",
      "Policy Evaluation Done!\n",
      "State Value Improved!\n",
      "-------- Policy Iteration 3 --------\n",
      "---- Delta:  12.2358814673903\n",
      "---- Delta:  6.294501099664103\n",
      "---- Delta:  3.1703732207261055\n",
      "---- Delta:  1.7284474568283485\n",
      "---- Delta:  1.1394031879518707\n",
      "---- Delta:  0.822034643224697\n",
      "---- Delta:  0.6064098440360794\n",
      "---- Delta:  0.4535350120302155\n",
      "Policy Evaluation Done!\n",
      "State Value Improved!\n",
      "-------- Policy Iteration 4 --------\n",
      "---- Delta:  3.5414453168573345\n",
      "---- Delta:  0.5641272921080827\n",
      "---- Delta:  0.338212895790889\n",
      "Policy Evaluation Done!\n",
      "State Value Improved!\n",
      "-------- Policy Iteration Done! --------\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent(env)\n",
    "agent.policy_iteration(theta = 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
